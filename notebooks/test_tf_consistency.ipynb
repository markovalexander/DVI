{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deterministic Variational Inference: Toy example\n",
    "Code to accompany [**Fixing Variational Bayes: Deterministic Variational Inference for Bayesian Neural Networks**](https://arxiv.org/abs/1810.03958)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=1\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import os, json \n",
    "\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "np.random.seed(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gaussian_variables as gv\n",
    "import utils as u\n",
    "import plot_utils as pu\n",
    "import bayes_layers as bnn\n",
    "from bayes_models import MLP, PointMLP, AdaptedMLP\n",
    "import bayes_util as bu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "First we generate a toy dataset according to:\n",
    "\n",
    "\\begin{equation}\n",
    "    y = -(x+0.5)\\sin(3\\pi x) + \\eta\n",
    "\\end{equation}\n",
    "\n",
    "Where the noise is generated according to:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\eta = 0.45(x + 0.5)^2\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def base_model(x):\n",
    "    return -(x+0.5)*np.sin(3 * np.pi *x)\n",
    "\n",
    "def noise_model(x):\n",
    "    return 0.45*(x+0.5)**2\n",
    "\n",
    "def sample_data(x):\n",
    "    return base_model(x) + np.random.normal(0, noise_model(x))\n",
    "\n",
    "data_size = {'train': 500, 'valid': 100, 'test': 100}\n",
    "toy_data = []\n",
    "for section in ['train', 'valid', 'test']:\n",
    "    x = (np.random.rand(data_size['train'], 1) - 0.5)\n",
    "    toy_data.append([x, sample_data(x).reshape(-1)])    \n",
    "x = np.arange(-1,1,1/100)\n",
    "toy_data.append([[[_] for _ in x], base_model(x)])\n",
    "\n",
    "pu.toy_results_plot(toy_data, {'mean':base_model, 'std':noise_model})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-bc05facdfe51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mLinearGaussian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     def __init__(self, in_features, out_features, certain=False,\n\u001b[1;32m      4\u001b[0m                  prior=\"DiagonalGaussian\"):\n\u001b[1;32m      5\u001b[0m         \"\"\"\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "class LinearGaussian(nn.Module):\n",
    "    def __init__(self, in_features, out_features, certain=False,\n",
    "                 prior=\"DiagonalGaussian\"):\n",
    "        \"\"\"\n",
    "        Applies linear transformation y = xA^T + b\n",
    "        A and b are Gaussian random variables\n",
    "        :param in_features: input dimension\n",
    "        :param out_features: output dimension\n",
    "        :param certain:  if false, than x is equal to its mean and has no variance\n",
    "        :param prior:  prior type\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.A_mean = nn.Parameter(torch.Tensor(in_features, out_features))\n",
    "        self.b_mean = nn.Parameter(torch.Tensor(out_features))\n",
    "        self.certain = certain\n",
    "\n",
    "        self.A_logvar = nn.Parameter(torch.Tensor(in_features, out_features))\n",
    "        self.b_logvar = nn.Parameter(torch.Tensor(out_features))\n",
    "\n",
    "        self.prior = prior\n",
    "        self.initialize_weights()\n",
    "        self.construct_priors(self.prior)\n",
    "        self.use_dvi = True\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        nn.init.xavier_normal_(self.A_mean)\n",
    "        nn.init.normal_(self.b_mean)\n",
    "\n",
    "        nn.init.xavier_normal_(self.A_logvar)\n",
    "        nn.init.normal_(self.b_logvar)\n",
    "\n",
    "    def construct_priors(self, prior):\n",
    "        if prior == \"DiagonalGaussian\":\n",
    "            s1 = s2 = 0.1\n",
    "\n",
    "            self._prior_A = {\n",
    "                'mean': torch.zeros_like(self.A_mean, requires_grad=False).to(2),\n",
    "                'var': torch.ones_like(self.A_logvar, requires_grad=False).to(2) * s2}\n",
    "            self._prior_b = {\n",
    "                'mean': torch.zeros_like(self.b_mean, requires_grad=False).to(2),\n",
    "                'var': torch.ones_like(self.b_logvar, requires_grad=False).to(2) * s1}\n",
    "        else:\n",
    "            raise NotImplementedError(\"{} prior is not supported\".format(prior))\n",
    "\n",
    "    def compute_kl(self):\n",
    "        if self.prior == 'DiagonalGaussian':\n",
    "            kl_A = KL_GG(self.A_mean, torch.exp(self.A_logvar),\n",
    "                         self._prior_A['mean'].to(self.A_mean.device),\n",
    "                         self._prior_A['var'].to(self.A_mean.device))\n",
    "            kl_b = KL_GG(self.b_mean, torch.exp(self.b_logvar),\n",
    "                         self._prior_b['mean'].to(self.A_mean.device),\n",
    "                         self._prior_b['var'].to(self.A_mean.device))\n",
    "        return kl_A + kl_b\n",
    "\n",
    "    def determenistic(self, mode=True):\n",
    "        self.use_dvi = True\n",
    "\n",
    "    def mcvi(self, mode=True):\n",
    "        self.use_dvi = not mode\n",
    "\n",
    "    def get_mode(self):\n",
    "        if self.use_dvi:\n",
    "            print('In determenistic mode')\n",
    "        else:\n",
    "            print('In MCVI mode')\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Compute expectation and variance after linear transform\n",
    "        y = xA^T + b\n",
    "        :param x: input, size [batch, in_features]\n",
    "        :return: tuple (y_mean, y_var) for determenistic mode:,  shapes:\n",
    "                 y_mean: [batch, out_features]\n",
    "                 y_var:  [batch, out_features, out_features]\n",
    "                 tuple (sample, None) for MCVI mode,\n",
    "                 sample : [batch, out_features] - local reparametrization of output\n",
    "        \"\"\"\n",
    "        A_var = torch.exp(self.A_logvar)\n",
    "        b_var = torch.exp(self.b_logvar)\n",
    "        if self.use_dvi:\n",
    "            return self._det_forward(x, A_var, b_var)\n",
    "        else:\n",
    "            return self._mcvi_forward(x, A_var, b_var)\n",
    "\n",
    "    def _mcvi_forward(self, x, A_var, b_var):\n",
    "        if self.certain:\n",
    "            x_mean = x\n",
    "            x_var = None\n",
    "        else:\n",
    "            x_mean = x[0]\n",
    "            x_var = x[1]\n",
    "\n",
    "        y_mean = F.linear(x_mean, self.A_mean.t()) + self.b_mean\n",
    "\n",
    "        if self.certain or not self.use_dvi:\n",
    "            xx = x_mean * x_mean\n",
    "            y_var = torch.diag_embed(F.linear(xx, A_var.t()) + b_var)\n",
    "        else:\n",
    "            y_var = self.compute_var(x_mean, x_var)\n",
    "\n",
    "        dst = MultivariateNormal(loc=y_mean, covariance_matrix=y_var)\n",
    "        sample = dst.rsample()\n",
    "        return sample, None\n",
    "\n",
    "    def _det_forward(self, x, A_var, b_var):\n",
    "        \"\"\"\n",
    "        Compute expectation and variance after linear transform\n",
    "        y = xA^T + b\n",
    "        :param x: input, size [batch, in_features]\n",
    "        :return: tuple (y_mean, y_var),  shapes:\n",
    "                 y_mean: [batch, out_features]\n",
    "                 y_var:  [batch, out_features, out_features]\n",
    "        \"\"\"\n",
    "\n",
    "        if self.certain:\n",
    "            x_mean = x\n",
    "            x_var = None\n",
    "        else:\n",
    "            x_mean = x[0]\n",
    "            x_var = x[1]\n",
    "\n",
    "        y_mean = F.linear(x_mean, self.A_mean.t()) + self.b_mean\n",
    "\n",
    "        if self.certain:\n",
    "            xx = x_mean * x_mean\n",
    "            y_var = torch.diag_embed(F.linear(xx, A_var.t()) + b_var)\n",
    "        else:\n",
    "            y_var = self.compute_var(x_mean, x_var)\n",
    "\n",
    "        return y_mean, y_var\n",
    "\n",
    "    def compute_var(self, x_mean, x_var):\n",
    "        A_var = torch.exp(self.A_logvar)\n",
    "\n",
    "        x_var_diag = matrix_diag_part(x_var)\n",
    "        xx_mean = x_var_diag + x_mean * x_mean\n",
    "\n",
    "        term1_diag = torch.matmul(xx_mean, A_var)\n",
    "\n",
    "        flat_xCov = torch.reshape(x_var, (-1, self.A_mean.size(0)))  # [b*x, x]\n",
    "        xCov_A = torch.matmul(flat_xCov, self.A_mean)  # [b * x, y]\n",
    "        xCov_A = torch.reshape(xCov_A, (\n",
    "            -1, self.A_mean.size(0), self.A_mean.size(1)))  # [b, x, y]\n",
    "        xCov_A = torch.transpose(xCov_A, 1, 2)  # [b, y, x]\n",
    "        xCov_A = torch.reshape(xCov_A, (-1, self.A_mean.size(0)))  # [b*y, x]\n",
    "\n",
    "        A_xCov_A = torch.matmul(xCov_A, self.A_mean)  # [b*y, y]\n",
    "        A_xCov_A = torch.reshape(A_xCov_A, (\n",
    "            -1, self.A_mean.size(1), self.A_mean.size(1)))  # [b, y, y]\n",
    "\n",
    "        term2 = A_xCov_A\n",
    "        term2_diag = matrix_diag_part(term2)\n",
    "\n",
    "        _, n, _ = term2.size()\n",
    "        idx = torch.arange(0, n)\n",
    "\n",
    "        term3_diag = torch.exp(self.b_logvar)\n",
    "        result_diag = term1_diag + term2_diag + term3_diag\n",
    "\n",
    "        result = term2\n",
    "        result[:, idx, idx] = result_diag\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сheck LinearCertain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_model = LinearGaussian(1, 3, certain=True)\n",
    "torch_loss = RegressionLoss(torch_model, use_heteroskedastic=False, homo_log_var_scale=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_mean = torch_model.A_mean.detach().numpy()\n",
    "\n",
    "A_var = torch_model.A_var.detach().numpy()\n",
    "\n",
    "b_mean = torch_model.b_mean.detach().numpy()\n",
    "b_var = torch_model.b_var.detach().numpy()\n",
    "\n",
    "class param():\n",
    "    def __init__(self, mean, var):\n",
    "        self.mean = mean\n",
    "        self.var = var\n",
    "\n",
    "A = param(A_mean, A_var)\n",
    "b = param(b_mean, b_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = torch.FloatTensor(toy_data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_mean, torch_var = torch_model(xx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now tf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_certain_activations(x_certain, A, b):\n",
    "    \"\"\"\n",
    "    compute y = x^T A + b\n",
    "    assuming x has zero variance\n",
    "    \"\"\"\n",
    "    x_mean = x_certain\n",
    "    xx = x_mean*x_mean\n",
    "    y_mean = tf.matmul(x_mean, A.mean) + b.mean\n",
    "    y_cov = tf.matrix_diag(tf.matmul(xx, A.var) + b.var)\n",
    "    return y_mean, y_cov\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x  = toy_data[0][0].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_mean, tf_var = linear_certain_activations(x, A, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=15, shape=(500, 3), dtype=float32, numpy=\n",
       "array([[ 0.91329587,  0.79197353,  0.28668857],\n",
       "       [ 0.9980265 ,  0.80330646,  0.39054853],\n",
       "       [ 0.77334714,  0.77325505,  0.11514421],\n",
       "       ...,\n",
       "       [ 0.96634424,  0.7990689 ,  0.35171345],\n",
       "       [ 0.65956944,  0.758037  , -0.0243206 ],\n",
       "       [ 1.0682223 ,  0.8126954 ,  0.4765921 ]], dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9133,  0.7920,  0.2867],\n",
       "        [ 0.9980,  0.8033,  0.3905],\n",
       "        [ 0.7733,  0.7733,  0.1151],\n",
       "        ...,\n",
       "        [ 0.9663,  0.7991,  0.3517],\n",
       "        [ 0.6596,  0.7580, -0.0243],\n",
       "        [ 1.0682,  0.8127,  0.4766]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=21, shape=(500, 3, 3), dtype=float32, numpy=\n",
       "array([[[5.8536243, 0.       , 0.       ],\n",
       "        [0.       , 6.416935 , 0.       ],\n",
       "        [0.       , 0.       , 4.5336475]],\n",
       "\n",
       "       [[6.17735  , 0.       , 0.       ],\n",
       "        [0.       , 6.4694743, 0.       ],\n",
       "        [0.       , 0.       , 4.832366 ]],\n",
       "\n",
       "       [[6.180491 , 0.       , 0.       ],\n",
       "        [0.       , 6.469984 , 0.       ],\n",
       "        [0.       , 0.       , 4.835264 ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[6.010252 , 0.       , 0.       ],\n",
       "        [0.       , 6.442355 , 0.       ],\n",
       "        [0.       , 0.       , 4.678176 ]],\n",
       "\n",
       "       [[7.237233 , 0.       , 0.       ],\n",
       "        [0.       , 6.641489 , 0.       ],\n",
       "        [0.       , 0.       , 5.8103747]],\n",
       "\n",
       "       [[6.743526 , 0.       , 0.       ],\n",
       "        [0.       , 6.5613623, 0.       ],\n",
       "        [0.       , 0.       , 5.3548055]]], dtype=float32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[5.8536, 0.0000, 0.0000],\n",
       "         [0.0000, 6.4169, 0.0000],\n",
       "         [0.0000, 0.0000, 4.5336]],\n",
       "\n",
       "        [[6.1774, 0.0000, 0.0000],\n",
       "         [0.0000, 6.4695, 0.0000],\n",
       "         [0.0000, 0.0000, 4.8324]],\n",
       "\n",
       "        [[6.1805, 0.0000, 0.0000],\n",
       "         [0.0000, 6.4700, 0.0000],\n",
       "         [0.0000, 0.0000, 4.8353]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[6.0103, 0.0000, 0.0000],\n",
       "         [0.0000, 6.4424, 0.0000],\n",
       "         [0.0000, 0.0000, 4.6782]],\n",
       "\n",
       "        [[7.2372, 0.0000, 0.0000],\n",
       "         [0.0000, 6.6415, 0.0000],\n",
       "         [0.0000, 0.0000, 5.8104]],\n",
       "\n",
       "        [[6.7435, 0.0000, 0.0000],\n",
       "         [0.0000, 6.5614, 0.0000],\n",
       "         [0.0000, 0.0000, 5.3548]]], grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now check Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "rreg = LinearGaussian(3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_mean = rreg.A_mean.detach().numpy()\n",
    "A_var = rreg.A_var.detach().numpy()\n",
    "\n",
    "b_mean = rreg.b_mean.detach().numpy()\n",
    "b_var = rreg.b_var.detach().numpy()\n",
    "\n",
    "class param():\n",
    "    def __init__(self, mean, var):\n",
    "        self.mean = mean\n",
    "        self.var = var\n",
    "        self.shape = mean.shape\n",
    "\n",
    "A = param(A_mean, A_var)\n",
    "b = param(b_mean, b_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = (torch_mean, torch_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx_tf = (torch_mean.detach().numpy(), torch_var.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_mean_1, torch_var_1 = rreg(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_covariance(x_mean, x_cov, A, b):\n",
    "    x_var_diag = tf.matrix_diag_part(x_cov)\n",
    "    xx_mean = x_var_diag + x_mean * x_mean\n",
    "    \n",
    "    term1_diag = tf.matmul(xx_mean, A.var)\n",
    "    \n",
    "    flat_xCov = tf.reshape(x_cov, [-1, A.shape[0]]) # [b*x, x]\n",
    "    xCov_A = tf.matmul(flat_xCov, A.mean) # [b*x, y]\n",
    "    xCov_A = tf.reshape(xCov_A, [-1, A.shape[0], A.shape[1]]) # [b, x, y]\n",
    "    xCov_A = tf.transpose(xCov_A, [0, 2, 1]) # [b, y, x]\n",
    "    xCov_A = tf.reshape(xCov_A, [-1, A.shape[0]]) # [b*y, x]\n",
    "    A_xCov_A = tf.matmul(xCov_A, A.mean) # [b*y, y]\n",
    "    A_xCov_A = tf.reshape(A_xCov_A, [-1, A.shape[1], A.shape[1]]) # [b, y, y]\n",
    "\n",
    "    term2 = A_xCov_A\n",
    "    term2_diag = tf.matrix_diag_part(term2)\n",
    "    \n",
    "    term3_diag = b.var\n",
    "    \n",
    "    result_diag = term1_diag + term2_diag + term3_diag\n",
    "    return tf.matrix_set_diag(term2, result_diag)      \n",
    "\n",
    "\n",
    "def linear(x, A, b):\n",
    "    \"\"\"\n",
    "    compute y = x^T A + b\n",
    "    \"\"\"\n",
    "    x_mean = x.mean\n",
    "    y_mean = tf.matmul(x_mean, A.mean) + b.mean\n",
    "    x_cov = x.var\n",
    "    y_cov = linear_covariance(x_mean, x_cov, A, b)\n",
    "    return y_mean, y_cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 2, 2)\n"
     ]
    }
   ],
   "source": [
    "tf_mean_1, tf_var_1 = linear(param(xx_tf[0], xx_tf[1]), A, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.2008, 0.9447],\n",
       "        [1.2895, 1.0223],\n",
       "        [1.0544, 0.8165],\n",
       "        [1.1783, 0.9250],\n",
       "        [1.3936, 1.1135],\n",
       "        [1.3955, 1.1151],\n",
       "        [0.9612, 0.7349],\n",
       "        [1.0073, 0.7752],\n",
       "        [0.9195, 0.6984],\n",
       "        [1.1389, 0.8904]], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_mean_1[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=337, shape=(10, 2), dtype=float32, numpy=\n",
       "array([[1.2008369 , 0.9447069 ],\n",
       "       [1.2894974 , 1.0223311 ],\n",
       "       [1.0543975 , 0.8164957 ],\n",
       "       [1.1783154 , 0.9249886 ],\n",
       "       [1.3936245 , 1.1134968 ],\n",
       "       [1.39551   , 1.1151475 ],\n",
       "       [0.96124655, 0.73493993],\n",
       "       [1.0072572 , 0.77522343],\n",
       "       [0.9194839 , 0.6983758 ],\n",
       "       [1.1388631 , 0.8904473 ]], dtype=float32)>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_mean_1[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[39.2456,  2.9597],\n",
       "         [ 2.9597, 32.7805]],\n",
       "\n",
       "        [[40.9367,  3.0715],\n",
       "         [ 3.0715, 34.0926]],\n",
       "\n",
       "        [[40.1750,  3.0726],\n",
       "         [ 3.0726, 33.2175]]], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_var_1[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=342, shape=(3, 2, 2), dtype=float32, numpy=\n",
       "array([[[39.245625 ,  2.959738 ],\n",
       "        [ 2.959738 , 32.78047  ]],\n",
       "\n",
       "       [[40.936665 ,  3.0714676],\n",
       "        [ 3.0714676, 34.0926   ]],\n",
       "\n",
       "       [[40.17499  ,  3.0725517],\n",
       "        [ 3.0725517, 33.21749  ]]], dtype=float32)>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_var_1[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Regression Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 982,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "EPS = 1e-8\n",
    "\n",
    "\n",
    "def matrix_diag_part(tensor):\n",
    "    return torch.stack(tuple(t.diag() for t in torch.unbind(tensor, 0)))\n",
    "\n",
    "\n",
    "class LinearGaussian(nn.Module):\n",
    "    def __init__(self, in_features, out_features, certain=False,\n",
    "                 prior=\"DiagonalGaussian\"):\n",
    "        \"\"\"\n",
    "        Applies linear transformation y = xA^T + b\n",
    "\n",
    "        A and b are Gaussian random variables\n",
    "\n",
    "        :param in_features: input dimension\n",
    "        :param out_features: output dimension\n",
    "        :param certain:  if false, than x is equal to its mean and has no variance\n",
    "        :param prior:  prior type\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.A_mean = nn.Parameter(torch.Tensor(in_features, out_features))\n",
    "        self.b_mean = nn.Parameter(torch.Tensor(out_features))\n",
    "        self.certain = certain\n",
    "\n",
    "        self.A_var = nn.Parameter(torch.Tensor(in_features, out_features))\n",
    "        self.b_var = nn.Parameter(torch.Tensor(out_features))\n",
    "\n",
    "        self.prior = prior\n",
    "        self.initialize_weights()\n",
    "        self.construct_priors(self.prior)\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        nn.init.zeros_(self.A_mean)\n",
    "        nn.init.zeros_(self.b_mean)\n",
    "\n",
    "        shape = self.b_var.size(0)\n",
    "        s = shape * shape\n",
    "\n",
    "        nn.init.uniform_(self.A_var, a=0, b=s)\n",
    "        nn.init.uniform_(self.b_var, a=0, b=s)\n",
    "\n",
    "    def construct_priors(self, prior):\n",
    "        if prior == \"DiagonalGaussian\":\n",
    "            s1 = 1\n",
    "            s2 = 1\n",
    "\n",
    "            self._prior_A = {\n",
    "                'mean': torch.zeros_like(self.A_mean, requires_grad=False),\n",
    "                'var': torch.ones_like(self.A_var, requires_grad=False) * s2}\n",
    "            self._prior_b = {\n",
    "                'mean': torch.zeros_like(self.b_mean, requires_grad=False),\n",
    "                'var': torch.ones_like(self.b_var, requires_grad=False) * s1}\n",
    "        else:\n",
    "            raise NotImplementedError(\"{} prior is not supported\".format(prior))\n",
    "\n",
    "    def compute_kl(self):\n",
    "        if self.prior == 'DiagonalGaussian':\n",
    "            kl_A = KL_GG(self.A_mean, self.A_var, self._prior_A['mean'],\n",
    "                         self._prior_A['var'])\n",
    "            kl_b = KL_GG(self.b_mean, self.b_var, self._prior_b['mean'],\n",
    "                         self._prior_b['var'])\n",
    "        return kl_A + kl_b\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Compute expectation and variance after linear transform\n",
    "        y = xA^T + b\n",
    "\n",
    "        :param x: input, size [batch, in_features]\n",
    "        :return: tuple (y_mean, y_var),  shapes:\n",
    "                 y_mean: [batch, out_features]\n",
    "                 y_var:  [batch, out_features, out_features]\n",
    "        \"\"\"\n",
    "\n",
    "        if self.certain:\n",
    "            x_mean = x\n",
    "            x_var = None\n",
    "        else:\n",
    "            x_mean = x[0]\n",
    "            x_var = x[1]\n",
    "\n",
    "        y_mean = F.linear(x_mean, self.A_mean.t()) + self.b_mean\n",
    "\n",
    "        if self.certain:\n",
    "            xx = x_mean * x_mean\n",
    "            y_var = torch.diag_embed(F.linear(xx, self.A_var.t()) + self.b_var)\n",
    "        else:\n",
    "            y_var = self.compute_var(x_mean, x_var)\n",
    "\n",
    "        return y_mean, y_var\n",
    "\n",
    "    def compute_var(self, x_mean, x_var):\n",
    "        x_var_diag = matrix_diag_part(x_var)\n",
    "        xx_mean = x_var_diag + x_mean * x_mean\n",
    "\n",
    "        term1_diag = torch.matmul(xx_mean, self.A_var)\n",
    "\n",
    "        flat_xCov = torch.reshape(x_var, (-1, self.A_mean.size(0)))  # [b*x, x]\n",
    "        xCov_A = torch.matmul(flat_xCov, self.A_mean)  # [b * x, y]\n",
    "        xCov_A = torch.reshape(xCov_A, (\n",
    "            -1, self.A_mean.size(0), self.A_mean.size(1)))  # [b, x, y]\n",
    "        xCov_A = torch.transpose(xCov_A, 1, 2)  # [b, y, x]\n",
    "        xCov_A = torch.reshape(xCov_A, (-1, self.A_mean.size(0)))  # [b*y, x]\n",
    "\n",
    "        A_xCov_A = torch.matmul(xCov_A, self.A_mean)  # [b*y, y]\n",
    "        A_xCov_A = torch.reshape(A_xCov_A, (\n",
    "            -1, self.A_mean.size(1), self.A_mean.size(1)))  # [b, y, y]\n",
    "\n",
    "        term2 = A_xCov_A\n",
    "        term2_diag = matrix_diag_part(term2)\n",
    "\n",
    "        _, n, _ = term2.size()\n",
    "        idx = torch.arange(0, n)\n",
    "\n",
    "        term3_diag = self.b_var\n",
    "        result_diag = term1_diag + term2_diag + term3_diag\n",
    "\n",
    "        result = term2\n",
    "        result[:, idx, idx] = result_diag\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 991,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RegressionLoss(nn.Module):\n",
    "    def __init__(self, net, method='bayes', use_heteroskedastic=False,\n",
    "                 homo_log_var_scale=1.):\n",
    "        \"\"\"\n",
    "        Compute ELBO for regression task\n",
    "\n",
    "        :param net: neural network\n",
    "        :param method:\n",
    "        :param use_heteroskedastic:\n",
    "        :param homo_log_var_scale:\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = net\n",
    "        self.method = method\n",
    "        self.use_het = use_heteroskedastic\n",
    "        if not self.use_het and homo_log_var_scale is None:\n",
    "            raise ValueError(\n",
    "                \"homo_log_var_scale must be set in homoskedastic mode\")\n",
    "\n",
    "        if self.use_het:\n",
    "            raise NotImplementedError(\"heterostadic is not supported yet\")\n",
    "\n",
    "        self.homo_log_var_scale = homo_log_var_scale\n",
    "\n",
    "    def gaussian_likelihood_core(self, target, mean, log_var, smm, sml, sll):\n",
    "        const = math.log(2 * math.pi)\n",
    "        exp = torch.exp(-log_var + 0.5 * sll)\n",
    "        return -0.5 * (const + log_var + exp * (smm + (mean - sml - target) ** 2))\n",
    "\n",
    "    def heteroskedastic_gaussian_loglikelihood(self, pred_mean, pred_var,\n",
    "                                               target):\n",
    "        log_var = pred_mean[:, 1].view(-1)\n",
    "        mean = pred_mean[:, 0].view(-1)\n",
    "\n",
    "        if self.method.lower() == 'bayes':\n",
    "            sll = pred_var[:, 1, 1].view(-1)\n",
    "            smm = pred_var[:, 0, 0].view(-1)\n",
    "            sml = pred_var[:, 0, 1].view(-1)\n",
    "        else:\n",
    "            sll = smm = sml = 0\n",
    "        return self.gaussian_likelihood_core(target, mean, log_var, smm, sml,\n",
    "                                             sll)\n",
    "\n",
    "    def homoskedastic_gaussian_loglikelihood(self, pred_mean, pred_var, target):\n",
    "        log_var = torch.FloatTensor([self.homo_log_var_scale], device=pred_mean.device)\n",
    "        mean = pred_mean[:, 0].view(-1)\n",
    "        sll = sml = 0\n",
    "        if self.method.lower() == 'bayes':\n",
    "            smm = pred_var[:, 0, 0].view(-1)\n",
    "        else:\n",
    "            smm = 0\n",
    "        return self.gaussian_likelihood_core(target, mean, log_var, smm, sml,\n",
    "                                         sll)\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred_mean = pred[0]\n",
    "        pred_var = pred[1]\n",
    "\n",
    "        assert not target.requires_grad\n",
    "        kl = 0.0\n",
    "        for module in self.net.children():\n",
    "            if hasattr(module, 'compute_kl'):\n",
    "                kl = kl + module.compute_kl()\n",
    "        if hasattr(self.net, 'compute_kl'):\n",
    "            kl = kl + self.net.compute_kl()\n",
    "\n",
    "        gaussian_likelihood = self.heteroskedastic_gaussian_loglikelihood if self.use_het \\\n",
    "            else self.homoskedastic_gaussian_loglikelihood\n",
    "\n",
    "        log_likelihood = gaussian_likelihood(pred_mean, pred_var, target)\n",
    "        batched_likelihood = torch.mean(log_likelihood)\n",
    "\n",
    "        loss = kl - batched_likelihood\n",
    "        return loss, batched_likelihood.detach(), kl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 992,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_reg = LinearGaussian(2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 993,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_mean = torch_reg.A_mean.detach().numpy()\n",
    "A_var = torch_reg.A_var.detach().numpy()\n",
    "\n",
    "b_mean = torch_reg.b_mean.detach().numpy()\n",
    "b_var = torch_reg.b_var.detach().numpy()\n",
    "\n",
    "class param():\n",
    "    def __init__(self, mean, var):\n",
    "        self.mean = mean\n",
    "        self.var = var\n",
    "        self.shape = mean.shape\n",
    "\n",
    "A = param(A_mean, A_var)\n",
    "b = param(b_mean, b_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 994,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_pred = torch_reg((torch_mean_1, torch_var_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 995,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.FloatTensor(toy_data[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 996,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = RegressionLoss(torch_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 997,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = criterion(torch_pred, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 998,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(8.7631, grad_fn=<SubBackward0>),\n",
       " tensor(-8.2606),\n",
       " tensor(0.5024, grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 998,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 999,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "tf_mean, tf_var = linear(param(tf_mean_1, tf_var_1), A, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1000,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[34.9562]],\n",
       "\n",
       "        [[36.5441]],\n",
       "\n",
       "        [[35.4090]]], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 1000,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_pred[1][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1001,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=4177, shape=(3, 1, 1), dtype=float32, numpy=\n",
       "array([[[34.956158]],\n",
       "\n",
       "       [[36.544125]],\n",
       "\n",
       "       [[35.409035]]], dtype=float32)>"
      ]
     },
     "execution_count": 1001,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_var[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1002,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiagonalGaussianVar(object):\n",
    "    def __init__(self, mean, var, shape=None):\n",
    "        self.mean = mean\n",
    "        self.var = var\n",
    "        self.shape = tf.shape(mean) if shape is None else shape\n",
    "    \n",
    "    def sample(self, n_sample=None):\n",
    "        no_sample_dim = False\n",
    "        if n_sample is None:\n",
    "            no_sample_dim = True\n",
    "            n_sample = 1\n",
    "        s = tf.random_normal(shape=tf.concat([[n_sample], self.shape], axis=0)) * tf.sqrt(self.var) + self.mean\n",
    "        if no_sample_dim:\n",
    "            return s[0,...]\n",
    "        else:\n",
    "            return s\n",
    "\n",
    "    def log_likelihood(self, x):\n",
    "        return -0.5 * (bu.log2pi + tf.log(self.var) + (x - self.mean)**2 / self.var)\n",
    "\n",
    "    \n",
    "def KL(p, q, hypers=None, global_step=1.0E99):\n",
    "    safe_qvar = q.var + bu.EPSILON\n",
    "    entropy_term = 0.5 * (1 + bu.log2pi + tf.log(p.var))\n",
    "\n",
    "    cross_entropy_term = 0.5 * (bu.log2pi + tf.log(safe_qvar) + (p.var + (p.mean - q.mean)**2) / safe_qvar)           \n",
    "    return tf.reduce_sum(cross_entropy_term - entropy_term)\n",
    "\n",
    "class Parameter(object):\n",
    "    def __init__(self, value, prior, variables=None):\n",
    "        self.value = value\n",
    "        self.prior = prior\n",
    "        self.variables = variables\n",
    "    \n",
    "    def surprise(self, hypers=None, global_step=1.0E99):\n",
    "        \"\"\"\n",
    "        compute KL(value || prior) \n",
    "        assuming\n",
    "            (1) diagonal gaussian value\n",
    "            (2) diagonal gaussian prior with scalar mu, var\n",
    "        \"\"\"\n",
    "        return KL(self.value, self.prior, hypers, global_step)\n",
    "\n",
    "    def log_likelihood(self):\n",
    "        return tf.reduce_sum(self.prior.log_likelihood(self.value.mean))\n",
    "\n",
    "    def standardize(self):\n",
    "        return DiagonalGaussianVar(\n",
    "            (self.value.mean - self.prior.mean) / np.sqrt(self.prior.var),\n",
    "            self.value.var / self.prior.var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1003,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heteroskedastic_gaussian_loglikelihood(pred, target, global_step, hypers):\n",
    "    log_variance = tf.reshape(pred.mean[:,1], [-1])\n",
    "    mean = tf.reshape(pred.mean[:,0], [-1])\n",
    "    if hypers['method'].lower().strip() == 'bayes':\n",
    "        sll = tf.reshape(pred.var[:,1,1], [-1])\n",
    "        smm = tf.reshape(pred.var[:,0,0], [-1])\n",
    "        sml = tf.reshape(pred.var[:,0,1], [-1])\n",
    "    else:\n",
    "        sll = smm = sml = tf.constant(0.0, dtype=tf.float32)\n",
    "    return gaussian_loglikelihood_core(target, mean, log_variance, smm, sml, sll)\n",
    "\n",
    "def homoskedastic_gaussian_loglikelihood(pred, target, global_step, hypers):\n",
    "    log_variance = tf.constant(hypers[\"homo_logvar_scale\"], dtype=tf.float32)\n",
    "    mean = tf.reshape(pred.mean[:,0], [-1])\n",
    "    sll = tf.constant(0.0, dtype=tf.float32)\n",
    "    sml = tf.constant(0.0, dtype=tf.float32)\n",
    "    if hypers['method'].lower().strip() == 'bayes':\n",
    "        smm = tf.reshape(pred.var[:,0,0], [-1])\n",
    "    else:\n",
    "        smm = tf.constant(0.0, dtype=tf.float32)\n",
    "    return gaussian_loglikelihood_core(target, mean, log_variance, smm, sml, sll)\n",
    "\n",
    "def gaussian_loglikelihood_core(target, mean, log_variance, smm, sml, sll):\n",
    "    \n",
    "    return -0.5 * (\n",
    "        bu.log2pi \n",
    "        + log_variance\n",
    "        + tf.exp(-log_variance + 0.5*sll)\n",
    "          * (smm + (mean - sml - target)**2)\n",
    "    )\n",
    "\n",
    "def regression_loss(pred, target, model, hypers, global_step):\n",
    "    all_surprise = tf.reduce_sum(tf.stack([w.surprise(hypers, global_step) for w in model.parameters]))\n",
    "    gaussian_loglikelihood = (     heteroskedastic_gaussian_loglikelihood \n",
    "                              if   hypers['style'] == 'heteroskedastic' \n",
    "                              else homoskedastic_gaussian_loglikelihood)\n",
    "    log_likelihood = gaussian_loglikelihood(pred, target, global_step, hypers)\n",
    "    batch_log_likelihood = tf.reduce_mean(log_likelihood)\n",
    "    L =  all_surprise - batch_log_likelihood\n",
    "    return L, batch_log_likelihood, all_surprise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### распределения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1004,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_A = DiagonalGaussianVar(tf.zeros_like(A.mean, dtype=tf.float32), \n",
    "                              tf.ones_like(A.var, dtype=tf.float32))\n",
    "\n",
    "prior_b = DiagonalGaussianVar(tf.zeros_like(b.mean, dtype=tf.float32),\n",
    "                             tf.ones_like(b.var, dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1005,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dst = DiagonalGaussianVar(tf_mean, tf_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1006,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_A = DiagonalGaussianVar(A.mean, A.var)\n",
    "value_b = DiagonalGaussianVar(b.mean, b.var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1007,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_A = Parameter(value_A, prior_A)\n",
    "param_b = Parameter(value_b, prior_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1008,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypers = {}\n",
    "hypers['style'] = 'homosked'\n",
    "hypers[\"homo_logvar_scale\"] = 1.\n",
    "hypers['method'] = 'bayes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1009,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model():\n",
    "    def __init__(self, params):\n",
    "        self.parameters = params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1010,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_model = model([param_A, param_b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1011,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_pred = param(tf_mean, tf_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1012,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: id=4264, shape=(), dtype=float32, numpy=8.763061>,\n",
       " <tf.Tensor: id=4263, shape=(), dtype=float32, numpy=-8.260625>,\n",
       " <tf.Tensor: id=4236, shape=(), dtype=float32, numpy=0.5024353>)"
      ]
     },
     "execution_count": 1012,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regression_loss(tf_pred, toy_data[0][1], tf_model, hypers, 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check KL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 862,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([0.], requires_grad=True), Parameter containing:\n",
       " tensor([0.3284], requires_grad=True))"
      ]
     },
     "execution_count": 862,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_reg.b_mean, torch_reg.b_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 863,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.], dtype=float32), array([0.3283553], dtype=float32))"
      ]
     },
     "execution_count": 863,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.mean, b.var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 864,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: id=3457, shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n",
       " <tf.Tensor: id=3460, shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>)"
      ]
     },
     "execution_count": 864,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prior_b.mean, prior_b.var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 870,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KL(p, q, hypers=None, global_step=1.0E99):\n",
    "    safe_qvar = q.var + bu.EPSILON\n",
    "    entropy_term = 0.5 * (1 + bu.log2pi + tf.log(p.var))\n",
    "    \n",
    "    \n",
    "    cross_entropy_term = 0.5 * (bu.log2pi + tf.log(safe_qvar) + (p.var + (p.mean - q.mean)**2) / safe_qvar) \n",
    "    return tf.reduce_sum(cross_entropy_term - entropy_term)\n",
    "\n",
    "\n",
    "def KL_GG(p_mean, p_var, q_mean, q_var):\n",
    "    \"\"\"\n",
    "    Computes KL (p || q) from p to q, assuming that both p and q have normal\n",
    "    distribution\n",
    "\n",
    "    :param p_mean:\n",
    "    :param p_var:\n",
    "    :param q_mean:\n",
    "    :param q_var:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    s_q_var = q_var + EPS\n",
    "    entropy = 0.5 * (1 + np.log(2 * np.pi) + torch.log(p_var))\n",
    "    \n",
    "    \n",
    "    cross_entropy = 0.5 * (math.log(2 * math.pi) + torch.log(s_q_var) + \\\n",
    "                           (p_var + (p_mean - q_mean) ** 2) / s_q_var)\n",
    "    return torch.sum(cross_entropy - entropy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=3601, shape=(), dtype=float32, numpy=0.22100753>"
      ]
     },
     "execution_count": 871,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KL(value_b, prior_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2210, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 872,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KL_GG(torch_reg.b_mean, torch_reg.b_var, torch_reg._prior_b['mean'], torch_reg._prior_b['var'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 875,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=3641, shape=(), dtype=float32, numpy=0.13841438>"
      ]
     },
     "execution_count": 875,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KL(value_A, prior_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 876,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1384, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 876,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KL_GG(torch_reg.A_mean, torch_reg.A_var, torch_reg._prior_A['mean'], torch_reg._prior_A['var'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### checl likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 877,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'homo_logvar_scale': 1.0, 'method': 'bayes', 'style': 'homosked'}"
      ]
     },
     "execution_count": 877,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 943,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heteroskedastic_gaussian_loglikelihood_tf(pred, target, global_step, hypers):\n",
    "    log_variance = tf.reshape(pred.mean[:,1], [-1])\n",
    "    mean = tf.reshape(pred.mean[:,0], [-1])\n",
    "    if hypers['method'].lower().strip() == 'bayes':\n",
    "        sll = tf.reshape(pred.var[:,1,1], [-1])\n",
    "        smm = tf.reshape(pred.var[:,0,0], [-1])\n",
    "        sml = tf.reshape(pred.var[:,0,1], [-1])\n",
    "    else:\n",
    "        sll = smm = sml = tf.constant(0.0, dtype=tf.float32)\n",
    "    return gaussian_loglikelihood_core_tf(target, mean, log_variance, smm, sml, sll)\n",
    "\n",
    "def homoskedastic_gaussian_loglikelihood_tf(pred, target, global_step, hypers):\n",
    "    log_variance = tf.constant(hypers[\"homo_logvar_scale\"], dtype=tf.float32)\n",
    "    mean = tf.reshape(pred.mean[:,0], [-1])\n",
    "    sll = tf.constant(0.0, dtype=tf.float32)\n",
    "    sml = tf.constant(0.0, dtype=tf.float32)\n",
    "    if hypers['method'].lower().strip() == 'bayes':\n",
    "        smm = tf.reshape(pred.var[:,0,0], [-1])\n",
    "    else:\n",
    "        smm = tf.constant(0.0, dtype=tf.float32)\n",
    "    return gaussian_loglikelihood_core_tf(target, mean, log_variance, smm, sml, sll)\n",
    "\n",
    "def gaussian_loglikelihood_core_tf(target, mean, log_variance, smm, sml, sll):\n",
    "    return -0.5 * (\n",
    "        bu.log2pi \n",
    "        + log_variance\n",
    "        + tf.exp(-log_variance + 0.5*sll)\n",
    "          * (smm + (mean - sml - target)**2)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 944,
   "metadata": {},
   "outputs": [],
   "source": [
    "class test_ll():\n",
    "    def __init__(self, method, logvar_scale):\n",
    "        self.method = method\n",
    "        self.logvar_scale = logvar_scale\n",
    "        \n",
    "    def gaussian_likelihood_core(self, target, mean, log_var, smm, sml, sll):\n",
    "        const = math.log(2 * math.pi)\n",
    "        exp = torch.exp(-log_var + 0.5 * sll)\n",
    "        return -0.5 * (const + log_var + exp * (smm + (mean - sml - target) ** 2))\n",
    "\n",
    "    def heteroskedastic_gaussian_loglikelihood(self, pred_mean, pred_var,\n",
    "                                               target):\n",
    "        log_var = pred_mean[:, 1].view(-1)\n",
    "        mean = pred_mean[:, 0].view(-1)\n",
    "\n",
    "        if self.method.lower() == 'bayes':\n",
    "            sll = pred_var[:, 1, 1].view(-1)\n",
    "            smm = pred_var[:, 0, 0].view(-1)\n",
    "            sml = pred_var[:, 0, 1].view(-1)\n",
    "        else:\n",
    "            sll = smm = sml = 0\n",
    "        return self.gaussian_likelihood_core(target, mean, log_var, smm, sml,\n",
    "                                             sll)\n",
    "\n",
    "    def homoskedastic_gaussian_loglikelihood(self, pred_mean, pred_var, target):\n",
    "        log_var = torch.FloatTensor([self.logvar_scale], device=pred_mean.device)\n",
    "        mean = pred_mean[:, 0].view(-1)\n",
    "        sll = sml = 0\n",
    "        if self.method.lower() == 'bayes':\n",
    "            smm = pred_var[:, 0, 0].view(-1)\n",
    "        else:\n",
    "            smm = 0\n",
    "        return self.gaussian_likelihood_core(target, mean, log_var, smm, sml,\n",
    "                                         sll)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 945,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_tester = test_ll('bayes', logvar_scale=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 949,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-11.6795,  -9.9893, -10.0081, -10.9889, -10.0269,  -9.9133],\n",
       "       grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 949,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll_tester.homoskedastic_gaussian_loglikelihood(torch_pred[0], torch_pred[1], torch.FloatTensor(toy_data[0][1]))[341:347]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 950,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=4022, shape=(6,), dtype=float32, numpy=\n",
       "array([-11.679482,  -9.989309, -10.008125, -10.988946, -10.026949,\n",
       "        -9.913277], dtype=float32)>"
      ]
     },
     "execution_count": 950,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "homoskedastic_gaussian_loglikelihood_tf(tf_pred, toy_data[0][1], 1e98, hypers)[341:347]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import utils as u\n",
    "import bayes_util as bu\n",
    "import gaussian_variables as gv\n",
    "\n",
    "EPSILON = 1e-6\n",
    "\n",
    "\n",
    "def linear(x, A, b):\n",
    "    \"\"\"\n",
    "    compute y = x^T A + b\n",
    "    \"\"\"\n",
    "    x_mean = x.mean\n",
    "    y_mean = tf.matmul(x_mean, A.mean) + b.mean\n",
    "    x_cov = x.var\n",
    "    y_cov = linear_covariance(x_mean, x_cov, A, b)\n",
    "    return gv.GaussianVar(y_mean, y_cov)\n",
    "\n",
    "def linear_certain_activations(x_certain, A, b):\n",
    "    \"\"\"\n",
    "    compute y = x^T A + b\n",
    "    assuming x has zero variance\n",
    "    \"\"\"\n",
    "    x_mean = x_certain\n",
    "    xx = x_mean*x_mean\n",
    "    y_mean = tf.matmul(x_mean, A.mean) + b.mean\n",
    "    y_cov = tf.matrix_diag(tf.matmul(xx, A.var) + b.var)\n",
    "    return y_mean, y_cov\n",
    "\n",
    "def linear_relu(x, A, b):\n",
    "    \"\"\"\n",
    "    compute y = relu(x)^T A + b\n",
    "    \"\"\"\n",
    "    x_var_diag = tf.matrix_diag_part(x.var)\n",
    "    sqrt_x_var_diag = tf.sqrt(x_var_diag)\n",
    "    mu = x.mean / (sqrt_x_var_diag + EPSILON)\n",
    "    \n",
    "    def relu_covariance(x):\n",
    "        mu1 = tf.expand_dims(mu, 2)\n",
    "        mu2 = tf.transpose(mu1, [0,2,1])\n",
    "\n",
    "        s11s22 = tf.expand_dims(x_var_diag, axis=2) * tf.expand_dims(x_var_diag, axis=1)\n",
    "        rho = x.var / (tf.sqrt(s11s22))# + EPSILON)\n",
    "        rho = tf.clip_by_value(rho, -1/(1+EPSILON), 1/(1+EPSILON))\n",
    "\n",
    "        return x.var * bu.delta(rho, mu1, mu2)   \n",
    "    \n",
    "    z_mean = sqrt_x_var_diag * bu.softrelu(mu)\n",
    "    y_mean = tf.matmul(z_mean, A.mean) + b.mean\n",
    "    z_cov = relu_covariance(x)\n",
    "    y_cov = linear_covariance(z_mean, z_cov, A, b)\n",
    "    return y_mean, y_cov\n",
    "\n",
    "def linear_relu_diagonal(x, A, b):\n",
    "    \"\"\"\n",
    "    compute y = relu(x)^T A + b\n",
    "    \"\"\"\n",
    "    x_var_diag = x.var\n",
    "    sqrt_x_var_diag = tf.sqrt(x_var_diag)\n",
    "    mu = x.mean / (sqrt_x_var_diag + EPSILON)\n",
    "    \n",
    "    pdf = bu.standard_gaussian(mu) \n",
    "    cdf = bu.gaussian_cdf(mu)\n",
    "    softrelu = pdf + mu*cdf\n",
    "    \n",
    "    z_mean = sqrt_x_var_diag * softrelu\n",
    "    y_mean = tf.matmul(z_mean, A.mean) + b.mean\n",
    "    z_var = x_var_diag * (cdf + mu*softrelu - tf.square(softrelu))\n",
    "    y_cov = linear_covariance_diagonal(z_mean, z_var, A, b)\n",
    "    return y_mean, y_cov\n",
    "\n",
    "def simple(x, A, b):\n",
    "    mu = x.mean\n",
    "    y_mean = tf.matmul(mu, A.mean) + b.mean    \n",
    "    y_cov = x.var\n",
    "    return gv.GaussianVar(y_mean, y_cov)\n",
    "\n",
    "\n",
    "def linear_heaviside(x, A, b):\n",
    "    \"\"\"\n",
    "    compute y = heaviside(x)^T A + b\n",
    "    \"\"\"\n",
    "    x_var_diag = tf.matrix_diag_part(x.var)\n",
    "    mu = x.mean / (tf.sqrt(x_var_diag) + EPSILON)\n",
    "    \n",
    "    def heaviside_covariance(x):\n",
    "        mu1 = tf.expand_dims(mu, 2)\n",
    "        mu2 = tf.transpose(mu1, [0,2,1])\n",
    "\n",
    "        s11s22 = tf.expand_dims(x_var_diag, axis=2) * tf.expand_dims(x_var_diag, axis=1)\n",
    "        rho = x.var / (tf.sqrt(s11s22))# + EPSILON)\n",
    "        rho = tf.clip_by_value(rho, -1/(1+EPSILON), 1/(1+EPSILON))\n",
    "\n",
    "        return bu.heavy_g(rho, mu1, mu2)\n",
    "    \n",
    "    z_mean = bu.gaussian_cdf(mu)\n",
    "    y_mean = tf.matmul(z_mean, A.mean) + b.mean\n",
    "    z_cov = heaviside_covariance(x)\n",
    "    y_cov = linear_covariance(z_mean, z_cov, A, b)\n",
    "    return y_mean, y_cov\n",
    "\n",
    "def linear_covariance_diagonal(x_mean, x_var, A, b):\n",
    "    xx_mean = x_var + x_mean * x_mean\n",
    "    term1_diag = tf.matmul(xx_mean, A.var)\n",
    "    Asqr = tf.square(A.mean)\n",
    "    A_xCov_A = tf.matmul(x_var, Asqr)\n",
    "    term2_diag = A_xCov_A\n",
    "    term3_diag = b.var\n",
    "    result_diag = term1_diag + term2_diag + term3_diag\n",
    "    return result_diag\n",
    "\n",
    "def linear_covariance(x_mean, x_cov, A, b):\n",
    "    x_var_diag = tf.matrix_diag_part(x_cov)\n",
    "    xx_mean = x_var_diag + x_mean * x_mean\n",
    "    \n",
    "    term1_diag = tf.matmul(xx_mean, A.var)\n",
    "    \n",
    "    flat_xCov = tf.reshape(x_cov, [-1, A.shape[0]]) # [b*x, x]\n",
    "    xCov_A = tf.matmul(flat_xCov, A.mean) # [b*x, y]\n",
    "    xCov_A = tf.reshape(xCov_A, [-1, A.shape[0], A.shape[1]]) # [b, x, y]\n",
    "    xCov_A = tf.transpose(xCov_A, [0, 2, 1]) # [b, y, x]\n",
    "    xCov_A = tf.reshape(xCov_A, [-1, A.shape[0]]) # [b*y, x]\n",
    "    A_xCov_A = tf.matmul(xCov_A, A.mean) # [b*y, y]\n",
    "    A_xCov_A = tf.reshape(A_xCov_A, [-1, A.shape[1], A.shape[1]]) # [b, y, y]\n",
    "\n",
    "    term2 = A_xCov_A\n",
    "    term2_diag = tf.matrix_diag_part(term2)\n",
    "    \n",
    "    term3_diag = b.var\n",
    "    \n",
    "    result_diag = term1_diag + term2_diag + term3_diag\n",
    "    return tf.matrix_set_diag(term2, result_diag)      \n",
    "\n",
    "def logsumexp(y, keepdims=False):\n",
    "    \"\"\"\n",
    "    compute <logsumexp(y)>\n",
    "    \"\"\"\n",
    "    lse = tf.reduce_logsumexp(y.mean, axis=-1, keep_dims=keepdims)   # [b, 1]\n",
    "    p = tf.exp(y.mean - lse)  # softmax                              # [b, y]\n",
    "    pTDiagVar = tf.reduce_sum(p * tf.matrix_diag_part(y.var), axis=-1, keep_dims=keepdims)        # [b, 1]\n",
    "    pTVarp = tf.squeeze(tf.matmul(tf.expand_dims(p, 1), tf.matmul(y.var, tf.expand_dims(p, 2))), axis=-1) # [b]\n",
    "    return lse + 0.5 * (pTDiagVar - pTVarp)\n",
    "\n",
    "def logsoftmax(y):\n",
    "    \"\"\"\n",
    "    compute <logsoftmax(y)>\n",
    "    \"\"\"\n",
    "    return y.mean - logsumexp(y, keepdims=True) # [b, y]\n",
    "\n",
    "def categorical_loss(logits, target, model, hypers, global_step, MC_samples=-1):\n",
    "    \"\"\"\n",
    "    compute <p(D|w)>_q - lambda KL(q || p)\n",
    "    \"\"\"\n",
    "    lsm = tf.cond(tf.greater(MC_samples, 0),\n",
    "        lambda: sampled_logsoftmax(logits, MC_samples), # we evaluate the logsoftmax using MC sampling\n",
    "        lambda: logsoftmax(logits)                      # we evaluate the logsoftmax using the delta approx\n",
    "    )\n",
    "\n",
    "    all_surprise = tf.reduce_sum(tf.stack([w.surprise() for w in model.parameters]))\n",
    "    logprob = tf.reduce_sum(target * lsm, axis=1)\n",
    "    batch_logprob = tf.reduce_mean(logprob)\n",
    "    \n",
    "    lmda = hypers['lambda']\n",
    "    \n",
    "    L = lmda * all_surprise / hypers['dataset_size'] - batch_logprob\n",
    "    return L, batch_logprob, all_surprise\n",
    "\n",
    "def heteroskedastic_gaussian_loglikelihood(pred, target, global_step, hypers):\n",
    "    log_variance = tf.reshape(pred.mean[:,1], [-1])\n",
    "    mean = tf.reshape(pred.mean[:,0], [-1])\n",
    "    if hypers['method'].lower().strip() == 'bayes':\n",
    "        sll = tf.reshape(pred.var[:,1,1], [-1])\n",
    "        smm = tf.reshape(pred.var[:,0,0], [-1])\n",
    "        sml = tf.reshape(pred.var[:,0,1], [-1])\n",
    "    else:\n",
    "        sll = smm = sml = tf.constant(0.0, dtype=tf.float32)\n",
    "    return gaussian_loglikelihood_core(target, mean, log_variance, smm, sml, sll)\n",
    "\n",
    "def homoskedastic_gaussian_loglikelihood(pred, target, global_step, hypers):\n",
    "    log_variance = tf.constant(hypers[\"homo_logvar_scale\"], dtype=tf.float32)\n",
    "    mean = tf.reshape(pred.mean[:,0], [-1])\n",
    "    sll = tf.constant(0.0, dtype=tf.float32)\n",
    "    sml = tf.constant(0.0, dtype=tf.float32)\n",
    "    if hypers['method'].lower().strip() == 'bayes':\n",
    "        smm = tf.reshape(pred.var[:,0,0], [-1])\n",
    "    else:\n",
    "        smm = tf.constant(0.0, dtype=tf.float32)\n",
    "    return gaussian_loglikelihood_core(target, mean, log_variance, smm, sml, sll)\n",
    "\n",
    "def gaussian_loglikelihood_core(target, mean, log_variance, smm, sml, sll):\n",
    "    return -0.5 * (\n",
    "        bu.log2pi \n",
    "        + log_variance\n",
    "        + tf.exp(-log_variance + 0.5*sll)\n",
    "          * (smm + (mean - sml - target)**2)\n",
    "    )\n",
    "\n",
    "def regression_loss(pred, target, model, hypers, global_step):\n",
    "    all_surprise = tf.reduce_sum(tf.stack([w.surprise(hypers, global_step) for w in model.parameters]))\n",
    "    gaussian_loglikelihood = (     heteroskedastic_gaussian_loglikelihood \n",
    "                              if   hypers['style'] == 'heteroskedastic' \n",
    "                              else homoskedastic_gaussian_loglikelihood)\n",
    "    log_likelihood = gaussian_loglikelihood(pred, target, global_step, hypers)\n",
    "    batch_log_likelihood = tf.reduce_mean(log_likelihood)\n",
    "    lmda = u.piecewise_anneal(hypers, 'lambda', global_step)\n",
    "    L = lmda * all_surprise / hypers['dataset_size'] - batch_log_likelihood\n",
    "    return L, batch_log_likelihood, all_surprise\n",
    "\n",
    "def point_catagorical_loss(logits, target, model, hypers, global_step, MC_samples=-1):\n",
    "    logprob = -tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=target))\n",
    "    return -logprob, logprob, tf.constant(0.0, dtype=tf.float32)\n",
    "\n",
    "def point_regression_loss(pred, target, model, hypers, global_step):\n",
    "    gaussian_loglikelihood = (     heteroskedastic_gaussian_loglikelihood \n",
    "                          if   hypers['style'] == 'heteroskedastic' \n",
    "                          else homoskedastic_gaussian_loglikelihood)\n",
    "    log_likelihood = gaussian_loglikelihood(pred, target, global_step, hypers)\n",
    "    batch_log_likelihood = tf.reduce_mean(log_likelihood)\n",
    "    if hypers['method'].lower().strip() == 'map':\n",
    "        all_LL = tf.reduce_sum(tf.stack([w.log_likelihood() for w in model.parameters]))\n",
    "        lmda = u.piecewise_anneal(hypers, 'lambda', global_step)\n",
    "        L = -lmda * all_LL / hypers['dataset_size'] - batch_log_likelihood\n",
    "    else:\n",
    "        all_LL = tf.constant(0)\n",
    "        L = -batch_log_likelihood\n",
    "    return L, batch_log_likelihood, all_LL\n",
    "\n",
    "def sample_activations(acts, n_sample):\n",
    "    \"\"\"\n",
    "    take n_sample samples from acts\n",
    "    input: acts: GaussianVar [batch_size (b), hidden size (h)]\n",
    "    \"\"\"\n",
    "    sigma_sqr = acts.var                                          # [b, h, h]\n",
    "    sigma = tf.transpose(tf.cholesky(sigma_sqr), [0,2,1])         # [b, h, h]\n",
    "    standard_samples = tf.random_normal(\n",
    "        [tf.shape(sigma)[0], n_sample, tf.shape(sigma)[-1]])      # [b, n_sample, h]\n",
    "    samples = tf.matmul(standard_samples, sigma) + tf.expand_dims(acts.mean, 1) # [b, n_sample, h]\n",
    "    return samples\n",
    "\n",
    "def sampled_logsoftmax(logits, n_sample):\n",
    "    samples = sample_activations(logits, n_sample)         # [b, n_sample, h]\n",
    "    softmax_samples = tf.nn.softmax(samples, dim=-1)       # [b, n_sample, h]\n",
    "    mean_softmax = tf.reduce_mean(softmax_samples, axis=1) # [b, h]\n",
    "    return tf.log(mean_softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "EPS = 1e-8\n",
    "\n",
    "\n",
    "def standard_gaussian(x):\n",
    "    return torch.exp(-1 / 2 * x * x) / np.sqrt(2 * math.pi)\n",
    "\n",
    "\n",
    "def gaussian_cdf(x):\n",
    "    const = 1 / np.sqrt(2)\n",
    "    return 0.5 * (1 + torch.erf(x * const))\n",
    "\n",
    "\n",
    "def softrelu(x):\n",
    "    return standard_gaussian(x) + x * gaussian_cdf(x)\n",
    "\n",
    "\n",
    "def heaviside_q(rho, mu1, mu2):\n",
    "    \"\"\"\n",
    "    Compute exp ( -Q(rho, mu1, mu2) ) for Heaviside activation\n",
    "\n",
    "    \"\"\"\n",
    "    rho_hat = torch.sqrt(1 - rho * rho)\n",
    "    arcsin = torch.asin(rho)\n",
    "\n",
    "    rho_s = torch.abs(rho) + EPS\n",
    "    arcsin_s = torch.abs(torch.asin(rho)) + EPS\n",
    "\n",
    "    A = arcsin / (2 * math.pi)\n",
    "    coef = rho_s / (2 * arcsin_s * rho_hat)\n",
    "    coefs_prod = (rho * rho) / (arcsin_s * rho_hat * (1 + rho_hat))\n",
    "    return A * torch.exp( -(mu1 * mu1 + mu2 * mu2) * coef + mu1 * mu2 * coefs_prod)\n",
    "\n",
    "\n",
    "def relu_q(rho, mu1, mu2):\n",
    "    \"\"\"\n",
    "    Compute exp ( -Q(rho, mu1, mu2) ) for ReLU activation\n",
    "\n",
    "    \"\"\"\n",
    "    rho_hat_plus_one = torch.sqrt(1 - rho * rho) + 1\n",
    "    g_r = torch.asin(rho) - rho / rho_hat_plus_one  # why minus?\n",
    "\n",
    "    rho_s = torch.abs(rho) + EPS\n",
    "    g_r_s = torch.abs(g_r) + EPS\n",
    "    A = g_r / (2 * math.pi)\n",
    "\n",
    "    coef_sum = rho_s / (2 * g_r_s * rho_hat_plus_one)\n",
    "    coef_prod = (torch.asin(rho) - rho) / (rho_s * g_r_s)\n",
    "    return A * torch.exp(\n",
    "        - (mu1 * mu1 + mu2 * mu2) * coef_sum + coef_prod * mu1 * mu2)\n",
    "\n",
    "\n",
    "def delta(rho, mu1, mu2):\n",
    "    return gaussian_cdf(mu1) * gaussian_cdf(mu2) + relu_q(rho, mu1, mu2)\n",
    "\n",
    "\n",
    "def KL_GG(p_mean, p_var, q_mean, q_var):\n",
    "    \"\"\"\n",
    "    Computes KL (p || q) from p to q, assuming that both p and q have normal\n",
    "    distribution\n",
    "\n",
    "    :param p_mean:\n",
    "    :param p_var:\n",
    "    :param q_mean:\n",
    "    :param q_var:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    s_q_var = q_var + EPS\n",
    "    entropy = 0.5 * (1 + math.log(2 * math.pi) + torch.log(p_var))\n",
    "    cross_entropy = 0.5 * (math.log(2 * math.pi) + torch.log(s_q_var) + \\\n",
    "                           (p_var + (p_mean - q_mean) ** 2) / s_q_var)\n",
    "    return torch.sum(cross_entropy - entropy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import MultivariateNormal\n",
    "\n",
    "EPS = 1e-8\n",
    "\n",
    "\n",
    "def matrix_diag_part(tensor):\n",
    "    return torch.stack(tuple(t.diag() for t in torch.unbind(tensor, 0)))\n",
    "\n",
    "\n",
    "class LinearGaussian(nn.Module):\n",
    "    def __init__(self, in_features, out_features, certain=False,\n",
    "                 prior=\"DiagonalGaussian\"):\n",
    "        \"\"\"\n",
    "        Applies linear transformation y = xA^T + b\n",
    "\n",
    "        A and b are Gaussian random variables\n",
    "\n",
    "        :param in_features: input dimension\n",
    "        :param out_features: output dimension\n",
    "        :param certain:  if false, than x is equal to its mean and has no variance\n",
    "        :param prior:  prior type\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.A_mean = nn.Parameter(torch.Tensor(in_features, out_features))\n",
    "        self.b_mean = nn.Parameter(torch.Tensor(out_features))\n",
    "        self.certain = certain\n",
    "\n",
    "        self.A_var = nn.Parameter(torch.Tensor(in_features, out_features))\n",
    "        self.b_var = nn.Parameter(torch.Tensor(out_features))\n",
    "\n",
    "        self.prior = prior\n",
    "        self.initialize_weights()\n",
    "        self.construct_priors(self.prior)\n",
    "        self.use_dvi = True\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        nn.init.xavier_normal_(self.A_mean)\n",
    "        nn.init.normal_(self.b_mean)\n",
    "\n",
    "        shape = self.b_var.size(0)\n",
    "        s = shape * shape\n",
    "\n",
    "        nn.init.uniform_(self.A_var, a=0, b=s)\n",
    "        nn.init.uniform_(self.b_var, a=0, b=s)\n",
    "        \n",
    "    def construct_priors(self, prior):\n",
    "        if prior == \"DiagonalGaussian\":\n",
    "            s1 = 1\n",
    "            s2 = 1\n",
    "\n",
    "            self._prior_A = {\n",
    "                'mean': torch.zeros_like(self.A_mean, requires_grad=False),\n",
    "                'var': torch.ones_like(self.A_var, requires_grad=False) * s2}\n",
    "            self._prior_b = {\n",
    "                'mean': torch.zeros_like(self.b_mean, requires_grad=False),\n",
    "                'var': torch.ones_like(self.b_var, requires_grad=False) * s1}\n",
    "        else:\n",
    "            raise NotImplementedError(\"{} prior is not supported\".format(prior))\n",
    "\n",
    "    def compute_kl(self):\n",
    "        if self.prior == 'DiagonalGaussian':\n",
    "            kl_A = KL_GG(self.A_mean, self.A_var, self._prior_A['mean'],\n",
    "                         self._prior_A['var'])\n",
    "            kl_b = KL_GG(self.b_mean, self.b_var, self._prior_b['mean'],\n",
    "                         self._prior_b['var'])\n",
    "        return kl_A + kl_b\n",
    "    \n",
    "    def determenistic(self, mode=True):\n",
    "        self.use_dvi = True\n",
    "    \n",
    "    def mcvi(self, mode=True):\n",
    "        self.use_dvi = not mode\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.use_dvi:\n",
    "            return self._det_forward(x)\n",
    "        else:\n",
    "            return self._mcvi_forward(x)\n",
    "    \n",
    "    def get_mode(self):\n",
    "        if self.use_dvi:\n",
    "            print('Using determenistic mode')\n",
    "        else:\n",
    "            print('Using MCVI')\n",
    "    \n",
    "    def _mcvi_forward(self, x):\n",
    "        if self.certain or not self.use_dvi:\n",
    "            x_mean = x\n",
    "            x_var = None\n",
    "        else:\n",
    "            x_mean = x[0]\n",
    "            x_var = x[1]\n",
    "            \n",
    "\n",
    "        y_mean = F.linear(x_mean, self.A_mean.t()) + self.b_mean\n",
    "        \n",
    "        if self.certain or not self.use_dvi:\n",
    "            xx = x_mean * x_mean\n",
    "            y_var = torch.diag_embed(F.linear(xx, self.A_var.t()) + self.b_var)\n",
    "        else:\n",
    "            y_var = self.compute_var(x_mean, x_var)\n",
    "    \n",
    "        dst = MultivariateNormal(loc=y_mean, covariance_matrix=y_var)\n",
    "        sample = dst.rsample()\n",
    "        return sample, None\n",
    "\n",
    "    def _det_forward(self, x):\n",
    "        \"\"\"\n",
    "        Compute expectation and variance after linear transform\n",
    "        y = xA^T + b\n",
    "\n",
    "        :param x: input, size [batch, in_features]\n",
    "        :return: tuple (y_mean, y_var),  shapes:\n",
    "                 y_mean: [batch, out_features]\n",
    "                 y_var:  [batch, out_features, out_features]\n",
    "        \"\"\"\n",
    "\n",
    "        if self.certain:\n",
    "            x_mean = x\n",
    "            x_var = None\n",
    "        else:\n",
    "            x_mean = x[0]\n",
    "            x_var = x[1]\n",
    "\n",
    "        y_mean = F.linear(x_mean, self.A_mean.t()) + self.b_mean\n",
    "\n",
    "        if self.certain:\n",
    "            xx = x_mean * x_mean\n",
    "            y_var = torch.diag_embed(F.linear(xx, self.A_var.t()) + self.b_var)\n",
    "        else:\n",
    "            y_var = self.compute_var(x_mean, x_var)\n",
    "\n",
    "        return y_mean, y_var\n",
    "\n",
    "    def compute_var(self, x_mean, x_var):\n",
    "        x_var_diag = matrix_diag_part(x_var)\n",
    "        xx_mean = x_var_diag + x_mean * x_mean\n",
    "\n",
    "        term1_diag = torch.matmul(xx_mean, self.A_var)\n",
    "\n",
    "        flat_xCov = torch.reshape(x_var, (-1, self.A_mean.size(0)))  # [b*x, x]\n",
    "        xCov_A = torch.matmul(flat_xCov, self.A_mean)  # [b * x, y]\n",
    "        xCov_A = torch.reshape(xCov_A, (\n",
    "            -1, self.A_mean.size(0), self.A_mean.size(1)))  # [b, x, y]\n",
    "        xCov_A = torch.transpose(xCov_A, 1, 2)  # [b, y, x]\n",
    "        xCov_A = torch.reshape(xCov_A, (-1, self.A_mean.size(0)))  # [b*y, x]\n",
    "\n",
    "        A_xCov_A = torch.matmul(xCov_A, self.A_mean)  # [b*y, y]\n",
    "        A_xCov_A = torch.reshape(A_xCov_A, (\n",
    "            -1, self.A_mean.size(1), self.A_mean.size(1)))  # [b, y, y]\n",
    "\n",
    "        term2 = A_xCov_A\n",
    "        term2_diag = matrix_diag_part(term2)\n",
    "\n",
    "        _, n, _ = term2.size()\n",
    "        idx = torch.arange(0, n)\n",
    "\n",
    "        term3_diag = self.b_var\n",
    "        result_diag = term1_diag + term2_diag + term3_diag\n",
    "\n",
    "        result = term2\n",
    "        result[:, idx, idx] = result_diag\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReluGaussian(nn.Module):\n",
    "    def __init__(self, in_features, out_features, certain=False,\n",
    "                 prior=\"DiagonalGaussian\"):\n",
    "        \"\"\"\n",
    "        Computes y = relu(x) * A.T + b\n",
    "\n",
    "        A and b are Gaussian random variables\n",
    "\n",
    "        :param in_features: input dimension\n",
    "        :param out_features: output dimension\n",
    "        :param certain:  if false, than x is equal to its mean and has no variance\n",
    "        :param prior:  prior type\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.linear = LinearGaussian(in_features, out_features, certain, prior)\n",
    "        self.certain = certain\n",
    "    \n",
    "    def compute_kl(self):\n",
    "        return self.linear.compute_kl()\n",
    "    \n",
    "    def determenistic(self, mode=True):\n",
    "        self.linear.use_dvi = True\n",
    "    \n",
    "    def mcvi(self, mode=True):\n",
    "        self.linear.use_dvi = not mode\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_mean = x[0]\n",
    "        x_var = x[1]\n",
    "        \n",
    "        x_var_diag = matrix_diag_part(x_var)\n",
    "        sqrt_x_var_diag = torch.sqrt(x_var_diag)\n",
    "        mu = x_mean / (sqrt_x_var_diag + EPS)\n",
    "        \n",
    "        z_mean = sqrt_x_var_diag * softrelu(mu)\n",
    "        z_var = self.compute_relu_var(x_var, x_var_diag, mu)\n",
    "        \n",
    "        return self.linear((z_mean, z_var))\n",
    "        \n",
    "    \n",
    "    def compute_relu_var(self, x_var, x_var_diag, mu):\n",
    "        mu1 = torch.unsqueeze(mu, 2)\n",
    "        mu2 = mu1.permute(0, 2, 1)\n",
    "        \n",
    "        s11s22 = torch.unsqueeze(x_var_diag, dim=2) * torch.unsqueeze(x_var_diag, dim=1)\n",
    "        rho = x_var / (torch.sqrt(s11s22) + EPS)\n",
    "        rho = torch.clamp(rho, -1/(1 + EPS), 1/(1 + EPS))\n",
    "        return x_var * delta(rho, mu1, mu2)\n",
    "        \n",
    "    \n",
    "    def get_mode(self):\n",
    "        if self.linear.use_dvi:\n",
    "            print('Using determenistic mode')\n",
    "        else:\n",
    "            print('Using MCVI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.6816, -1.8824, -0.9251]], grad_fn=<AddBackward0>),\n",
       " tensor([[[ 3.3855e+01, -6.5271e-02, -2.2865e-02],\n",
       "          [-6.8984e-02,  4.0166e+01,  5.7750e-02],\n",
       "          [-2.7098e-02,  6.3768e-02,  3.9443e+01]]],\n",
       "        grad_fn=<AsStridedBackward>))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.FloatTensor([[[1, 2, 3], [3, 4, 5]], [[4, 5, 6], [6, 7, 8]]])\n",
    "relu = ReluGaussian(in_features=2, out_features=3)\n",
    "inp = (torch.randn(1, 2), torch.abs(torch.randn(1, 2, 2)))\n",
    "relu(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx_tf = (inp[0].detach().numpy(), inp[1].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_mean = relu.linear.A_mean.detach().numpy()\n",
    "A_var = relu.linear.A_var.detach().numpy()\n",
    "\n",
    "b_mean = relu.linear.b_mean.detach().numpy()\n",
    "b_var = relu.linear.b_var.detach().numpy()\n",
    "\n",
    "class param():\n",
    "    def __init__(self, mean, var):\n",
    "        self.mean = mean\n",
    "        self.var = var\n",
    "        self.shape = mean.shape\n",
    "\n",
    "A = param(A_mean, A_var)\n",
    "b = param(b_mean, b_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = linear_relu(param(inp[0].detach().numpy(), inp[1].detach().numpy()), A, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=35, shape=(1, 3), dtype=float32, numpy=array([[ 0.681599 , -1.882401 , -0.9250962]], dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=116, shape=(1, 3, 3), dtype=float32, numpy=\n",
       "array([[[ 3.3855083e+01, -6.5270990e-02, -2.2865180e-02],\n",
       "        [-6.8984307e-02,  4.0165447e+01,  5.7749651e-02],\n",
       "        [-2.7097866e-02,  6.3768245e-02,  3.9442703e+01]]], dtype=float32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeavisideGaussian(nn.Module):\n",
    "    def __init__(self, in_features, out_features, certain=False,\n",
    "                 prior=\"DiagonalGaussian\"):\n",
    "        \"\"\"\n",
    "        Computes y = relu(x) * A.T + b\n",
    "\n",
    "        A and b are Gaussian random variables\n",
    "\n",
    "        :param in_features: input dimension\n",
    "        :param out_features: output dimension\n",
    "        :param certain:  if false, than x is equal to its mean and has no variance\n",
    "        :param prior:  prior type\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.linear = LinearGaussian(in_features, out_features, certain, prior)\n",
    "        self.certain = certain\n",
    "    \n",
    "    def compute_kl(self):\n",
    "        return self.linear.compute_kl()\n",
    "    \n",
    "    def determenistic(self, mode=True):\n",
    "        self.linear.use_dvi = True\n",
    "    \n",
    "    def mcvi(self, mode=True):\n",
    "        self.linear.use_dvi = not mode\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_mean = x[0]\n",
    "        x_var = x[1]\n",
    "        \n",
    "        x_var_diag = matrix_diag_part(x_var)\n",
    "        \n",
    "        sqrt_x_var_diag = torch.sqrt(x_var_diag)\n",
    "        mu = x_mean / (sqrt_x_var_diag + EPS)\n",
    "        \n",
    "        z_mean = gaussian_cdf(mu)\n",
    "        z_var = self.compute_var(x_var, x_var_diag, mu)\n",
    "        return self.linear((z_mean, z_var))\n",
    "        \n",
    "    \n",
    "    def compute_var(self, x_var, x_var_diag, mu):\n",
    "        mu1 = torch.unsqueeze(mu, 2)\n",
    "        mu2 = mu1.permute(0, 2, 1)\n",
    "        \n",
    "        s11s22 = torch.unsqueeze(x_var_diag, dim=2) * torch.unsqueeze(x_var_diag, dim=1)\n",
    "        rho = x_var / torch.sqrt(s11s22)\n",
    "        rho = torch.clamp(rho, -1/(1 + 1e-6), 1/(1 + 1e-6))\n",
    "        return heaviside_q(rho, mu1, mu2)\n",
    "        \n",
    "    \n",
    "    def get_mode(self):\n",
    "        if self.linear.use_dvi:\n",
    "            print('Using determenistic mode')\n",
    "        else:\n",
    "            print('Using MCVI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def standard_gaussian(x):\n",
    "    return torch.exp(-1 / 2 * x * x) / np.sqrt(2 * math.pi)\n",
    "\n",
    "\n",
    "def gaussian_cdf(x):\n",
    "    const = 1 / np.sqrt(2)\n",
    "    return 0.5 * (1 + torch.erf(x * const))\n",
    "\n",
    "\n",
    "def softrelu(x):\n",
    "    return standard_gaussian(x) + x * gaussian_cdf(x)\n",
    "\n",
    "\n",
    "def heaviside_q(rho, mu1, mu2):\n",
    "    \"\"\"\n",
    "    Compute exp ( -Q(rho, mu1, mu2) ) for Heaviside activation\n",
    "\n",
    "    \"\"\"\n",
    "    rho_hat = torch.sqrt(1 - rho * rho)\n",
    "    arcsin = torch.asin(rho)\n",
    "\n",
    "    rho_s = torch.abs(rho) + EPS\n",
    "    arcsin_s = torch.abs(torch.asin(rho)) + EPS/2 \n",
    "    \n",
    "    A = arcsin / (2 * math.pi)\n",
    "    one_over_coef_sum = (2 * arcsin_s * rho_hat) / rho_s\n",
    "    one_over_coefs_prod = (arcsin_s * rho_hat * (1 + rho_hat)) / (rho * rho)\n",
    "    return A * torch.exp( -(mu1 * mu1 + mu2 * mu2) / one_over_coef_sum + mu1 * mu2 / one_over_coefs_prod)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inp = (torch.randn(1, 2), torch.abs(torch.randn(1, 2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.6090, -0.8820,  0.7017]], grad_fn=<AddBackward0>),\n",
       " tensor([[[ 9.4264, -0.0645,  0.0710],\n",
       "          [-0.0644,  7.1350, -0.0980],\n",
       "          [ 0.0710, -0.0980,  4.8127]]], grad_fn=<AsStridedBackward>))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heav = HeavisideGaussian(in_features=2, out_features=3)\n",
    "heav(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=221, shape=(1, 3), dtype=float32, numpy=array([[-0.60896283, -0.88199776,  0.70169187]], dtype=float32)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=290, shape=(1, 3, 3), dtype=float32, numpy=\n",
       "array([[[ 9.426151  , -0.06442882,  0.07099799],\n",
       "        [-0.06441835,  7.1345396 , -0.09795044],\n",
       "        [ 0.07099719, -0.09796521,  4.8122907 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_mean = heav.linear.A_mean.detach().numpy()\n",
    "A_var = heav.linear.A_var.detach().numpy()\n",
    "\n",
    "b_mean = heav.linear.b_mean.detach().numpy()\n",
    "b_var = heav.linear.b_var.detach().numpy()\n",
    "\n",
    "class param():\n",
    "    def __init__(self, mean, var):\n",
    "        self.mean = mean\n",
    "        self.var = var\n",
    "        self.shape = mean.shape\n",
    "\n",
    "A = param(A_mean, A_var)\n",
    "b = param(b_mean, b_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = tf.constant(np.pi, dtype=tf.float32)\n",
    "sqrt2 = tf.constant(np.sqrt(2.0), dtype=tf.float32)\n",
    "twopi = tf.constant(2.0 * np.pi, dtype=tf.float32)\n",
    "sqrt2pi = tf.constant(np.sqrt(2.0 * np.pi), dtype=tf.float32)\n",
    "one_ovr_sqrt2pi = tf.constant(1.0 / np.sqrt(2.0 * np.pi), dtype=tf.float32)\n",
    "one_ovr_sqrt2 = tf.constant(1.0 / np.sqrt(2.0), dtype=tf.float32)\n",
    "log2pi = tf.constant(np.log(2.0 * np.pi), dtype=tf.float32)\n",
    "EPSILON = tf.constant(1e-6)\n",
    "HALF_EPSILON = EPSILON / 2.0\n",
    "\n",
    "def standard_gaussian(x):\n",
    "    return one_ovr_sqrt2pi * tf.exp(-x*x / 2.0)\n",
    "\n",
    "def gaussian_cdf(x):\n",
    "    return 0.5 * (1.0 + tf.erf(x * one_ovr_sqrt2))\n",
    "\n",
    "def softrelu(x):\n",
    "    return standard_gaussian(x) + x * gaussian_cdf(x)\n",
    "\n",
    "\n",
    "## exp ( - Q(rho, mu1, mu2)) для Heaviside nonlinearity\n",
    "def heavy_g(rho, mu1, mu2):\n",
    "    sqrt_one_minus_rho_sqr = tf.sqrt(1.0 - rho*rho)\n",
    "    a = tf.asin(rho)\n",
    "    \n",
    "    safe_a = tf.abs(a) + HALF_EPSILON\n",
    "    safe_rho = tf.abs(rho) + EPSILON\n",
    "\n",
    "    A = a / twopi\n",
    "    sxx = safe_a * sqrt_one_minus_rho_sqr / safe_rho\n",
    "    sxy = safe_a * sqrt_one_minus_rho_sqr * (1 + sqrt_one_minus_rho_sqr) / (rho * rho)\n",
    "    return A * tf.exp(-(mu1*mu1 + mu2*mu2) / (2.0 * sxx) + mu1*mu2/sxy)\n",
    "\n",
    "\n",
    "def linear_heaviside(x, A, b):\n",
    "    \"\"\"\n",
    "    compute y = heaviside(x)^T A + b\n",
    "    \"\"\"\n",
    "    x_var_diag = tf.matrix_diag_part(x.var)\n",
    "    mu = x.mean / (tf.sqrt(x_var_diag) + EPSILON)\n",
    "    \n",
    "    def heaviside_covariance(x):\n",
    "        mu1 = tf.expand_dims(mu, 2)\n",
    "        mu2 = tf.transpose(mu1, [0,2,1])\n",
    "\n",
    "        s11s22 = tf.expand_dims(x_var_diag, axis=2) * tf.expand_dims(x_var_diag, axis=1)\n",
    "        rho = x.var / (tf.sqrt(s11s22))# + EPSILON)\n",
    "        rho = tf.clip_by_value(rho, -1/(1+EPSILON), 1/(1+EPSILON))\n",
    "        return heavy_g(rho, mu1, mu2)\n",
    "    \n",
    "    z_mean = bu.gaussian_cdf(mu)\n",
    "    y_mean = tf.matmul(z_mean, A.mean) + b.mean\n",
    "    z_cov = heaviside_covariance(x)\n",
    "    y_cov = linear_covariance(z_mean, z_cov, A, b)\n",
    "    return y_mean, y_cov\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = linear_heaviside(param(inp[0].detach().numpy(), inp[1].detach().numpy()), A, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=221, shape=(1, 3), dtype=float32, numpy=array([[-0.60896283, -0.88199776,  0.70169187]], dtype=float32)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=290, shape=(1, 3, 3), dtype=float32, numpy=\n",
       "array([[[ 9.426151  , -0.06442882,  0.07099799],\n",
       "        [-0.06441835,  7.1345396 , -0.09795044],\n",
       "        [ 0.07099719, -0.09796521,  4.8122907 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsumexp_mean(y):\n",
    "    \"\"\"\n",
    "    Compute <logsumexp(y)>\n",
    "    :param y: tuple of (y_mean, y_var)\n",
    "    y_mean dim [batch_size, hid_dim]\n",
    "    y_var dim  [batch_size, hid_dim, hid_dim]\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    y_mean = y[0]\n",
    "    y_var = y[1]\n",
    "    logsumexp = F.log_softmax(y_mean, dim=-1)\n",
    "    p = torch.exp(y_mean - logsumexp)\n",
    "\n",
    "    pTdiagVar = torch.sum(p * matrix_diag_part(y_var), dim=-1)\n",
    "    pTVarp = torch.squeeze(torch.mn(torch.unsqueeze(p, 1), torch.mn(y_var, torch.unsqueeze(p, 2))), dim=-1)\n",
    "    return logsumexp + 0.5 * (pTdiagVar - pTVarp)\n",
    "\n",
    "\n",
    "def logsoftmax_mean(y):\n",
    "    \"\"\"\n",
    "    Compute <logsoftmax(y)>\n",
    "    :param y:\n",
    "    :param y: tuple of (y_mean, y_var)\n",
    "    y_mean dim [batch_size, hid_dim]\n",
    "    y_var dim  [batch_size, hid_dim, hid_dim]\n",
    "    \"\"\"\n",
    "    return y[0] - logsumexp_mean(y)\n",
    "\n",
    "\n",
    "def forward(self, logits, target, model, step):\n",
    "    \"\"\"\n",
    "    Compute <log p(y | D)> - kl\n",
    "    :param logits: shape [batch_size, n_classes]\n",
    "    :param target: shape [batch_size, n_classes] -- one-hot target\n",
    "    :param model:\n",
    "    :param step:\n",
    "    :return:\n",
    "        total loss\n",
    "        batch_logprob term\n",
    "        total kl term\n",
    "    \"\"\"\n",
    "    logsoftmax = logsoftmax_mean(logits)\n",
    "\n",
    "    logprob = torch.sum(target * logsoftmax, axis=1)\n",
    "    batch_logprob = torch.mean(logprob)\n",
    "\n",
    "    return batch_logprob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsumexp(y, keepdims=False):\n",
    "    \"\"\"\n",
    "    compute <logsumexp(y)>\n",
    "    \"\"\"\n",
    "    lse = tf.reduce_logsumexp(y.mean, axis=-1, keep_dims=keepdims)   # [b, 1]\n",
    "    p = tf.exp(y.mean - lse)  # softmax                              # [b, y]\n",
    "    pTDiagVar = tf.reduce_sum(p * tf.matrix_diag_part(y.var), axis=-1, keep_dims=keepdims)        # [b, 1]\n",
    "    pTVarp = tf.squeeze(tf.matmul(tf.expand_dims(p, 1), tf.matmul(y.var, tf.expand_dims(p, 2))), axis=-1) # [b]\n",
    "    return lse + 0.5 * (pTDiagVar - pTVarp)\n",
    "\n",
    "def logsoftmax(y):\n",
    "    \"\"\"\n",
    "    compute <logsoftmax(y)>\n",
    "    \"\"\"\n",
    "    return y.mean - logsumexp(y, keepdims=True) # [b, y]\n",
    "\n",
    "def categorical_loss(logits, target, model, hypers, global_step, MC_samples=-1):\n",
    "    \"\"\"\n",
    "    compute <p(D|w)>_q - lambda KL(q || p)\n",
    "    \"\"\"\n",
    "    lsm = logsoftmax(logits)                      # we evaluate the logsoftmax using the delta approx\n",
    "\n",
    "\n",
    "    logprob = tf.reduce_sum(target * lsm, axis=1)\n",
    "    batch_logprob = tf.reduce_mean(logprob)\n",
    "\n",
    "    return batch_logprob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class param():\n",
    "    def __init__(self, mean, var):\n",
    "        self.mean = mean\n",
    "        self.var = var\n",
    "        self.shape = mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[0.5, 0.1, 0, -1], [0.3, 0.3, -10, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
